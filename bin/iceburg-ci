#!/usr/bin/env bash
set -eo pipefail
PIPELINE_HOME="$( cd "$(dirname "$0")/.." ; pwd -P)"
export PIPELINE_HOME

__ENTRY=$(basename "$0")
readonly __ENTRY

log(){ printf "[%-5s%s $__ENTRY] $*\\n" "${__ll:-INFO}" >&2; }
die(){ __ll=ERROR log "$*"; exit 1; }
dlog(){ __ll=DEBUG log "$*"; }

lib/help(){
  lib/version
  log "iCEBURG-CI Step Usage: iceburg-ci [step...]"
  log "              ex: bin/ci $DEFAULT_STEPS"
  log "More help: ${ICEBURG_CI_URL:-https://github.com/iceburgci/iceburg-ci}"
}

lib/version(){
  log "iCEBURG-CI version: $(git -C "$PIPELINE_HOME" rev-parse --short HEAD) ($(git -C "$PIPELINE_HOME" rev-parse --abbrev-ref HEAD))"
}

DEFAULT_STEPS="${DEFAULT_STEPS:-check build test}"

case "$1" in
  -h|--help|help)
    lib/help
    exit
    ;;
  -v|--version|version)
    lib/version
    exit
    ;;
  *)
    [ $# -eq 0 ] && set -- $DEFAULT_STEPS
    ;;
esac

# remain backwards compatible with PROJECT_ROOT if PROJECT_HOME is not set.
export PROJECT_HOME=${PROJECT_HOME:-$PROJECT_ROOT}
[ -n "$PROJECT_HOME" ] || die "PROJECT_HOME cannot be empty"
cd "$PROJECT_HOME" || die "failed entering PROJECT_HOME"

#
# the CI platform should provide these variables. these are fallback defaults
# that are used when building locally or if the CI platform does not provide.
#
# the pipeline ID provides a unique namespace, e.g. <branch name>-<build number>
#   typically used for versioning and images tags
export PIPELINE_ID="${PIPELINE_ID:-local-0}"
export PROJECT_NAME="${PROJECT_NAME:-$(basename "$PROJECT_HOME")}"
export MANIFEST_FILE="${PIPELINE_MANIFEST:-$PROJECT_HOME/ci/manifest.json}"

# modeled from OCI image spec:
#  https://github.com/opencontainers/image-spec/blob/master/annotations.md
export __AUTHORS="${__AUTHORS:-$(git log --format="%aN <%aE>" -n 1 HEAD)}"
export __CREATED="${__CREATED:-$(date -u +"%Y-%m-%dT%H:%M:%SZ")}"
export __REVISION="${__REVISION:-$(git rev-parse HEAD)}"
export __SOURCE="${__SOURCE:-$(git remote get-url origin)}"
export __URL="${__URL:-localhost}"
export __VERSION="${__VERSION:-unknown}"

#
# step execution
#

export COMPOSE_PROJECT_NAME="${PROJECT_NAME}-${PIPELINE_ID}"
export COMPOSE_DOCKER_CLI_BUILD=1
export DOCKER_BUILDKIT=1
export DOCKER_CONFIG="${DOCKER_CONFIG:-$HOME/.docker}"
export DOCKER_SCAN_SUGGEST=false
docker info --format '{{.OperatingSystem}}' | grep -q -e 'Docker Desktop' -e 'Docker for Mac' && __DOCKER_DESKTOP=true || __DOCKER_DESKTOP=false

compose_flags=(
  -f "ci/docker-compose.yml"
)

run_flags=(
  --rm
  -v "/var/run/docker.sock:/var/run/docker.sock"
  -v "$DOCKER_CONFIG:$DOCKER_CONFIG"
  -v "$PIPELINE_HOME:$PIPELINE_HOME:ro"
  -l "net.iceburg.ci.id=$PIPELINE_ID"
  -e "COMPOSE_PROJECT_NAME=${COMPOSE_PROJECT_NAME}-0"
  -e "PIPELINE_STEP"
)

# platform customizations
if [ "$GITLAB_CI" = true ]; then
  dlog "Platform: GitLab CI"
  # when inside a gitlab ci docker executor, detect+mount the builds volume.
  # an alternate and more simple solution is to modify docker runner to bind-mount /builds, e.g.
  #  gitlab-runner register \
  #    ...
  #    --docker-volumes '/var/run/docker.sock:/var/run/docker.sock'
  #    --docker-volumes '/builds:/builds'
  CONTAINER_ID=$(docker ps -q -f "label=com.gitlab.gitlab-runner.job.id=$CI_JOB_ID" -f "label=com.gitlab.gitlab-runner.type=build")
  if [ -n "$CONTAINER_ID" ]; then
    log "detected docker executor"
    BUILDS_VOLUME=$(docker inspect "$CONTAINER_ID" -f "{{ range .Mounts }}{{ if eq .Destination \"/builds\" }}{{ .Source }}{{end}}{{end}}" | cut -d "/" -f 6)
    if [ -n "$BUILDS_VOLUME" ]; then
      export PROJECT_HOME="$BUILDS_VOLUME:/builds"
    fi
  fi
elif $__DOCKER_DESKTOP; then
  dlog "Platform: Docker Desktop"
  DOCKER_HOST=$(docker context inspect -f "{{ .Endpoints.docker.Host }}" default)
  export DOCKER_HOST
  export SSH_AUTH_SOCK=/run/host-services/ssh-auth.sock
else
  dlog "Platform: Unknown"
fi

run_flags+=(
  --workdir "$PROJECT_HOME"
  -v "$PROJECT_HOME:$PROJECT_HOME"
)

# passthrough the ssh auth socket if set
[ -n "$SSH_AUTH_SOCK" ] && run_flags+=(
  -v "$SSH_AUTH_SOCK:$SSH_AUTH_SOCK"
)

# passthrough the /shared volume if it exists
[ -d /shared ] && run_flags+=(
  -v "/shared:/shared"
)

# make sure we can mount AWS_SHARED_CREDENTIALS_FILE if it's specified
[ -e "$AWS_SHARED_CREDENTIALS_FILE" ] && run_flags+=(
  -v "$AWS_SHARED_CREDENTIALS_FILE:$AWS_SHARED_CREDENTIALS_FILE"
)

# passthrough runtime environment with exception of certain variables
env_blocklist=(
  -u _
  -u COMPOSE_PROJECT_NAME
  -u EDITOR
  -u GEM_HOME
  -u GOPATH
  -u HOME
  -u HOSTNAME
  -u JAVA_HOME
  -u JDK_HOME
  -u LANG
  -u LC_CTYPE
  -u LC_TYPE
  -u LOGNAME
  -u MAIL
  -u OLDPWD
  -u PATH
  -u PWD
  -u SHELL
  -u SHLVL
  -u TERM
  -u TERM_SESSION_ID
  -u TMPDIR
  -u USER
)
for envar in $(env "${env_blocklist[@]}" | cut -f1 -d= | sed '/.*[[:space:]]/d'); do
  run_flags+=( -e "$envar" )
done

compose(){
  docker-compose "${compose_flags[@]}" "$@"
}

manifest(){
  "$PIPELINE_HOME/bin/manifest" "$@" || __ll=WARN log "manifest $1 $2 operation failed"
}

trap '{
  RV=$?
  dlog "tearing down..."
  compose down --remove-orphans -v &>/dev/null
  COMPOSE_PROJECT_NAME=${COMPOSE_PROJECT_NAME}-0 compose down --remove-orphans -v &>/dev/null
  exit $RV
}' EXIT

for step in "$@"; do
  export PIPELINE_STEP="$step"

  log "Building $step step..."
  compose build -q --pull "$step"

  log "Running $step step..."
  manifest step start
  compose run "${run_flags[@]}" "$step" || {
    manifest step stop
    die "step failure: $step"
  }
  manifest step stop
done
